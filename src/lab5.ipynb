{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the data \n",
    "data = pd.read_csv('../data/datingData/training.txt', header = None, sep = \"\\t\" )\n",
    "data.columns = ['flyer_miles', 'video_game_time', 'litres_iceCream', 'type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flyer_miles</th>\n",
       "      <th>video_game_time</th>\n",
       "      <th>litres_iceCream</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40920</td>\n",
       "      <td>8.326976</td>\n",
       "      <td>0.953952</td>\n",
       "      <td>largeDoses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14488</td>\n",
       "      <td>7.153469</td>\n",
       "      <td>1.673904</td>\n",
       "      <td>smallDoses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26052</td>\n",
       "      <td>1.441871</td>\n",
       "      <td>0.805124</td>\n",
       "      <td>didntLike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75136</td>\n",
       "      <td>13.147394</td>\n",
       "      <td>0.428964</td>\n",
       "      <td>didntLike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38344</td>\n",
       "      <td>1.669788</td>\n",
       "      <td>0.134296</td>\n",
       "      <td>didntLike</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flyer_miles  video_game_time  litres_iceCream        type\n",
       "0        40920         8.326976         0.953952  largeDoses\n",
       "1        14488         7.153469         1.673904  smallDoses\n",
       "2        26052         1.441871         0.805124   didntLike\n",
       "3        75136        13.147394         0.428964   didntLike\n",
       "4        38344         1.669788         0.134296   didntLike"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 600 entries, 0 to 599\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   flyer_miles      600 non-null    int64  \n",
      " 1   video_game_time  600 non-null    float64\n",
      " 2   litres_iceCream  600 non-null    float64\n",
      " 3   type             600 non-null    object \n",
      "dtypes: float64(2), int64(1), object(1)\n",
      "memory usage: 18.9+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['largeDoses', 'smallDoses', 'didntLike'], dtype=object)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the types of people\n",
    "data['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of original data :  (600, 4)\n",
      "shape of x_train       :  (600, 3)\n",
      "shape of y_train       :  (600,)\n"
     ]
    }
   ],
   "source": [
    "#separting the data into x_train, y_train\n",
    "print(\"shape of original data : \", data.shape)\n",
    "x_train = data[data.columns[0:3]]\n",
    "y_train = data['type']\n",
    "print(\"shape of x_train       : \", x_train.shape)\n",
    "print(\"shape of y_train       : \", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting labels into Integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the class labels from categorical to integer \n",
    "mask0 = y_train == 'largeDoses'\n",
    "mask1 = y_train == 'smallDoses'\n",
    "mask2 = y_train == 'didntLike'\n",
    "\n",
    "y_train[mask0] = 0\n",
    "y_train[mask1] = 1\n",
    "y_train[mask2] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 220, 0: 200, 1: 180})\n"
     ]
    }
   ],
   "source": [
    "#checking if the class labels are balanced\n",
    "from collections import Counter \n",
    "print(Counter(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flyer_miles</th>\n",
       "      <th>video_game_time</th>\n",
       "      <th>litres_iceCream</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9916</td>\n",
       "      <td>2.695935</td>\n",
       "      <td>1.512111</td>\n",
       "      <td>smallDoses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38889</td>\n",
       "      <td>3.356646</td>\n",
       "      <td>0.324230</td>\n",
       "      <td>didntLike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39075</td>\n",
       "      <td>14.677836</td>\n",
       "      <td>0.793183</td>\n",
       "      <td>largeDoses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48071</td>\n",
       "      <td>1.551934</td>\n",
       "      <td>0.130902</td>\n",
       "      <td>didntLike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7275</td>\n",
       "      <td>2.464739</td>\n",
       "      <td>0.223502</td>\n",
       "      <td>smallDoses</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flyer_miles  video_game_time  litres_iceCream        type\n",
       "0         9916         2.695935         1.512111  smallDoses\n",
       "1        38889         3.356646         0.324230   didntLike\n",
       "2        39075        14.677836         0.793183  largeDoses\n",
       "3        48071         1.551934         0.130902   didntLike\n",
       "4         7275         2.464739         0.223502  smallDoses"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the test dataset\n",
    "data_test = pd.read_csv('../data/datingData/test.txt', header = None, sep = \"\\t\" )\n",
    "data_test.columns = ['flyer_miles', 'video_game_time', 'litres_iceCream', 'type']\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of test data    :  (400, 4)\n",
      "shape of x_test       :  (400, 3)\n",
      "shape of y_test       :  (400,)\n"
     ]
    }
   ],
   "source": [
    "#separating the test_dataset into x_test and y_test\n",
    "print(\"shape of test data    : \", data_test.shape)\n",
    "x_test = data_test[data.columns[0:3]]\n",
    "y_test = data_test['type']\n",
    "print(\"shape of x_test       : \", x_test.shape)\n",
    "print(\"shape of y_test       : \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 151, 0: 127, 2: 122})\n"
     ]
    }
   ],
   "source": [
    "#converting the class labels from categorical to integer \n",
    "mask0 = y_test == 'largeDoses'\n",
    "mask1 = y_test == 'smallDoses'\n",
    "mask2 = y_test == 'didntLike'\n",
    "\n",
    "y_test[mask0] = 0\n",
    "y_test[mask1] = 1\n",
    "y_test[mask2] = 2\n",
    "\n",
    "print(Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Classification using Multi-Label Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.092000e+04 8.326976e+00 9.539520e-01]\n",
      " [1.448800e+04 7.153469e+00 1.673904e+00]\n",
      " [2.605200e+04 1.441871e+00 8.051240e-01]]\n",
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 3)\n",
      "(600,)\n",
      "<class 'numpy.int32'>\n",
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "y_train = y_train.astype('int')\n",
    "print(type(y_train[0]))\n",
    "print(type(x_train[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.63333333 0.61666667 0.66666667 0.56666667 0.66666667 0.81666667\n",
      " 0.65       0.9        0.58333333 0.73333333]\n",
      "Range of Scores:  0.57 0.9\n",
      "Mean:  68.33333333333333\n"
     ]
    }
   ],
   "source": [
    "#define the model \n",
    "model = LogisticRegression(multi_class = 'multinomial', solver = 'lbfgs')\n",
    "\n",
    "#repeated random test-train splits\n",
    "cv_kFold = KFold(n_splits = 10, random_state = 77)\n",
    "\n",
    "#scoring \n",
    "scores = cross_val_score(model, x_train, y_train, cv = cv_kFold)\n",
    "print(scores)\n",
    "print(\"Range of Scores: \", np.min(scores).round(2), np.max(scores).round(2))\n",
    "print(\"Mean: \", scores.mean()*100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Here we can observe that our scores range from 57% to 90%, which means that our model doesn't deliver good accuracy on all sections of data** \n",
    "\n",
    "\n",
    "- **To reduce this range and have a more consistant performance, we will perform some hyper-paramter tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.93333333 0.88333333 0.88333333 0.88333333 0.9        0.86666667\n",
      " 0.95       0.95       0.95       0.91666667]\n",
      "Range of Scores:  0.87 0.95\n",
      "Mean:  91.16666666666666\n"
     ]
    }
   ],
   "source": [
    "#changing the solver \n",
    "model = LogisticRegression(multi_class = 'multinomial', solver = 'newton-cg')\n",
    "\n",
    "#repeated random test-train splits\n",
    "cv_kFold = KFold(n_splits = 10, random_state = 77)\n",
    "\n",
    "#scoring \n",
    "scores = cross_val_score(model, x_train, y_train, cv = cv_kFold)\n",
    "print(scores)\n",
    "print(\"Range of Scores: \", np.min(scores).round(2), np.max(scores).round(2))\n",
    "print(\"Mean: \", scores.mean()*100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Now we can observe our scores range from 87% to 95% which is much smaller range than before. We also have a better mean accuracy** \n",
    "\n",
    "\n",
    "- **Hence, we can safely conclude that the model with updated hyperparameters is better and more consistant on different sections of data** \n",
    "\n",
    "\n",
    "- **Therfore, we will now fit the model using the same parameters on whole training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='newton-cg', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting model on whole training data\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test dataset:  92.75\n"
     ]
    }
   ],
   "source": [
    "#Testing on test_data\n",
    "result = model.score(x_test, y_test)\n",
    "print(\"Accuracy on test dataset: \" , result * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[110   1  16]\n",
      " [  3 148   0]\n",
      " [  6   3 113]]\n"
     ]
    }
   ],
   "source": [
    "#get the confusion matrix \n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.87      0.89       127\n",
      "           1       0.97      0.98      0.98       151\n",
      "           2       0.88      0.93      0.90       122\n",
      "\n",
      "    accuracy                           0.93       400\n",
      "   macro avg       0.92      0.92      0.92       400\n",
      "weighted avg       0.93      0.93      0.93       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_pred, digits = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9238683464109699\n"
     ]
    }
   ],
   "source": [
    "#print the f1 score \n",
    "from sklearn.metrics import f1_score \n",
    "\n",
    "print(f1_score(y_test, y_pred, average = 'macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Classification using KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73333333 0.71666667 0.78333333 0.7        0.73333333 0.78333333\n",
      " 0.9        0.8        0.83333333 0.8       ]\n",
      "Range of Scores:  0.7 0.9\n",
      "Mean:  77.83333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model2 = KNeighborsClassifier(n_neighbors = 3)\n",
    "\n",
    "#repeated random test-train splits\n",
    "cv_kFold = KFold(n_splits = 10, random_state = 77)\n",
    "\n",
    "#scoring \n",
    "scores = cross_val_score(model2, x_train, y_train, cv = cv_kFold)\n",
    "print(scores)\n",
    "print(\"Range of Scores: \", np.min(scores).round(2), np.max(scores).round(2))\n",
    "print(\"Mean: \", scores.mean()*100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **We observe that the range of accuracy scores on the validation set ranges from 70% to 90%.** \n",
    "\n",
    "\n",
    "- **The mean accuracy score is ~78%** \n",
    "\n",
    "\n",
    "- **Let us see if we can reduce the range and increase the mean accuracy by tuning the number of neighbors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75       0.76666667 0.78333333 0.68333333 0.8        0.76666667\n",
      " 0.86666667 0.8        0.88333333 0.83333333]\n",
      "Range of Scores:  0.68 0.88\n",
      "Mean:  79.33333333333333\n"
     ]
    }
   ],
   "source": [
    "model2 = KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "#repeated random test-train splits\n",
    "cv_kFold = KFold(n_splits = 10, random_state = 77)\n",
    "\n",
    "#scoring \n",
    "scores = cross_val_score(model2, x_train, y_train, cv = cv_kFold)\n",
    "print(scores)\n",
    "print(\"Range of Scores: \", np.min(scores).round(2), np.max(scores).round(2))\n",
    "print(\"Mean: \", scores.mean()*100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We improved the mean accuracy but we didn't observe too much diffirence in the range. Experimentation continues**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7        0.78333333 0.78333333 0.71666667 0.78333333 0.73333333\n",
      " 0.85       0.81666667 0.85       0.76666667]\n",
      "Range of Scores:  0.7 0.85\n",
      "Mean:  77.83333333333331\n"
     ]
    }
   ],
   "source": [
    "model2 = KNeighborsClassifier(n_neighbors = 6 )\n",
    "\n",
    "#repeated random test-train splits\n",
    "cv_kFold = KFold(n_splits = 10, random_state = 77)\n",
    "\n",
    "#scoring \n",
    "scores = cross_val_score(model2, x_train, y_train, cv = cv_kFold)\n",
    "print(scores)\n",
    "print(\"Range of Scores: \", np.min(scores).round(2), np.max(scores).round(2))\n",
    "print(\"Mean: \", scores.mean()*100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **We can observe that the best results were observed using `n_neighbors = 5`** \n",
    "\n",
    "\n",
    "- **Using that to train on the whole dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting model on whole training data\n",
    "model2 = KNeighborsClassifier(n_neighbors = 5)\n",
    "model2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test dataset:  80.25\n"
     ]
    }
   ],
   "source": [
    "#Testing on test_data\n",
    "result = model2.score(x_test, y_test)\n",
    "print(\"Accuracy on test dataset: \" , result * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 88   5  34]\n",
      " [  6 145   0]\n",
      " [ 34   0  88]]\n"
     ]
    }
   ],
   "source": [
    "#get the confusion matrix \n",
    "y_pred = model2.predict(x_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.69      0.69       127\n",
      "           1       0.97      0.96      0.96       151\n",
      "           2       0.72      0.72      0.72       122\n",
      "\n",
      "    accuracy                           0.80       400\n",
      "   macro avg       0.79      0.79      0.79       400\n",
      "weighted avg       0.80      0.80      0.80       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_pred, digits = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7916542344476234\n"
     ]
    }
   ],
   "source": [
    "#print the f1 score \n",
    "from sklearn.metrics import f1_score \n",
    "\n",
    "print(f1_score(y_test, y_pred, average = 'macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Classification using Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95       0.91666667 0.91666667 0.95       0.91666667 0.93333333\n",
      " 0.98333333 0.93333333 0.98333333 0.95      ]\n",
      "Range of Scores:  0.92 0.98\n",
      "Mean:  94.33333333333331\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model3 = DecisionTreeClassifier()\n",
    "\n",
    "#repeated random test-train splits\n",
    "cv_kFold = KFold(n_splits = 10, random_state = 77)\n",
    "\n",
    "#scoring \n",
    "scores = cross_val_score(model3, x_train, y_train, cv = cv_kFold)\n",
    "print(scores)\n",
    "print(\"Range of Scores: \", np.min(scores).round(2), np.max(scores).round(2))\n",
    "print(\"Mean: \", scores.mean()*100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **We observe that the range of accuracies on validation set is from 92% to 98% with mean accuracy of 94.3%** \n",
    "\n",
    "\n",
    "- **let us see if we can improve the accuracy using the hyper-parameter tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.93333333 0.91666667 0.91666667 0.96666667 0.91666667 0.95\n",
      " 1.         0.93333333 0.98333333 0.95      ]\n",
      "Range of Scores:  0.92 1.0\n",
      "Mean:  94.66666666666666\n"
     ]
    }
   ],
   "source": [
    "model3 = DecisionTreeClassifier(max_depth = 7)\n",
    "\n",
    "#repeated random test-train splits\n",
    "cv_kFold = KFold(n_splits = 10, random_state = 77)\n",
    "\n",
    "#scoring \n",
    "scores = cross_val_score(model3, x_train, y_train, cv = cv_kFold)\n",
    "print(scores)\n",
    "print(\"Range of Scores: \", np.min(scores).round(2), np.max(scores).round(2))\n",
    "print(\"Mean: \", scores.mean()*100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96666667 0.95       0.93333333 0.95       0.93333333 0.93333333\n",
      " 1.         0.93333333 1.         0.98333333]\n",
      "Range of Scores:  0.93 1.0\n",
      "Mean:  95.83333333333333\n"
     ]
    }
   ],
   "source": [
    "model3 = DecisionTreeClassifier(max_depth = 4)\n",
    "\n",
    "#repeated random test-train splits\n",
    "cv_kFold = KFold(n_splits = 10, random_state = 77)\n",
    "\n",
    "#scoring \n",
    "scores = cross_val_score(model3, x_train, y_train, cv = cv_kFold)\n",
    "print(scores)\n",
    "print(\"Range of Scores: \", np.min(scores).round(2), np.max(scores).round(2))\n",
    "print(\"Mean: \", scores.mean()*100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95       0.9        0.91666667 0.95       0.9        0.91666667\n",
      " 0.98333333 0.93333333 0.98333333 0.95      ]\n",
      "Range of Scores:  0.9 0.98\n",
      "Mean:  93.83333333333333\n"
     ]
    }
   ],
   "source": [
    "model3 = DecisionTreeClassifier(max_depth = 10)\n",
    "\n",
    "#repeated random test-train splits\n",
    "cv_kFold = KFold(n_splits = 10, random_state = 77)\n",
    "\n",
    "#scoring \n",
    "scores = cross_val_score(model3, x_train, y_train, cv = cv_kFold)\n",
    "print(scores)\n",
    "print(\"Range of Scores: \", np.min(scores).round(2), np.max(scores).round(2))\n",
    "print(\"Mean: \", scores.mean()*100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **We observe that we get the best accuracy with `max_depth = 4`** \n",
    "\n",
    "\n",
    "- **Using this setting to fit on entire training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=4, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = DecisionTreeClassifier(max_depth = 4)\n",
    "\n",
    "model3.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test dataset:  95.75\n"
     ]
    }
   ],
   "source": [
    "#Testing on test_data\n",
    "result = model3.score(x_test, y_test)\n",
    "print(\"Accuracy on test dataset: \" , result * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[115   3   9]\n",
      " [  4 146   1]\n",
      " [  0   0 122]]\n"
     ]
    }
   ],
   "source": [
    "#get the confusion matrix \n",
    "y_pred = model3.predict(x_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.93       127\n",
      "           1       0.98      0.97      0.97       151\n",
      "           2       0.92      1.00      0.96       122\n",
      "\n",
      "    accuracy                           0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_pred, digits = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9563075347288906\n"
     ]
    }
   ],
   "source": [
    "#print the f1 score \n",
    "from sklearn.metrics import f1_score \n",
    "\n",
    "print(f1_score(y_test, y_pred, average = 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we tested 3 methods: \n",
    "\n",
    "- Logistic regression\n",
    "\n",
    "- K-Nearest Neighbors\n",
    "\n",
    "- Decision Trees \n",
    "\n",
    "\n",
    "to categorize the customer into 3 categories: \n",
    "\n",
    "- People he/she didnâ€™t like\n",
    "\n",
    "- People he/she liked in small doses\n",
    "\n",
    "- People he/she liked in large doses\n",
    "\n",
    "\n",
    "Using attributes: \n",
    "\n",
    "- Number of frequent flyer miles earned per year\n",
    "\n",
    "- Percentage of time spent playing video games\n",
    "\n",
    "- Liters of ice cream consumed per week.\n",
    "\n",
    "\n",
    "**Best results for Accuracy and F1 scores obtained after hyper-paramter tuning of each method are summarized below**\n",
    "\n",
    "#### Accuracy \n",
    "\n",
    "- Logistic Regression: 92.75%\n",
    "\n",
    "\n",
    "- K-Nearest Neighbor : 80.25% \n",
    "\n",
    "\n",
    "- Decision Trees     : 95.75%\n",
    "\n",
    "#### F1 - scores\n",
    "\n",
    "- Logistic Regression: 92.4%\n",
    "\n",
    "\n",
    "- K-Nearest Neighbor : 79.20% \n",
    "\n",
    "\n",
    "- Decision Trees     : 95.60%\n",
    "\n",
    "\n",
    "*Using the accuracy scores and F1 scores, we conclude the the Decision Tree classifier performs the best and would be our classifier of choice for this experiment. Logistic regresion classifier also comes close to the Decision Tree classifier but being a simpler model, it is not able to handle the complexity of data as well as the Decision Tree model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
